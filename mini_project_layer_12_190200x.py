# -*- coding: utf-8 -*-
"""Mini_Project_Layer_12_190200X.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1elvvVKVdWF_5wJjMcqLjahQK9-9-nD-c

#Layer 12
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

names=[]
# Assign column names to the dataset
for i in range(1,769):
 names.append("feature_"+str(i))
labels=["label_1","label_2","label_3","label_4"]
names+=labels

# Read in the dataset
train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Mini_project/DataSet_L12/train.csv')
valid_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Mini_project/DataSet_L12/valid.csv')
test_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Mini_project/DataSet_L12/test.csv')

from sklearn.preprocessing import RobustScaler

x_train ={}
y_train ={}
x_valid ={}
y_valid ={}
x_test = {}



for label in labels:
  scaler = RobustScaler()
  df_t = train_df
  df_v = valid_df
  if label == 'label_2':
    df_t = train_df.dropna()
    df_v = valid_df.dropna()

  x_train[label] = scaler.fit_transform(df_t.drop(labels, axis=1))
  y_train[label] = df_t[label]
  x_valid[label] = scaler.transform(df_v.drop(labels, axis=1))
  y_valid[label] = df_v[label]
  x_test[label] = scaler.transform(test_df.drop(['ID'], axis=1))

"""##Label 1

###KNN
"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=5)
classifier.fit(x_train['label_1'], y_train['label_1'])

y_pred = classifier.predict(x_valid['label_1'])

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_valid['label_1'], y_pred))
print(classification_report(y_valid['label_1'], y_pred))

"""###SVM"""

from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report

# Create an SVM classifier with your desired hyperparameters
svm_classifier = SVC(class_weight='balanced' ,C=10, degree=1, gamma='auto', kernel='poly')

# Perform k-fold cross-validation (e.g., with k=5)
scores = cross_val_score(svm_classifier, x_train['label_1'], y_train['label_1'], cv=5, scoring='accuracy')

# Print the cross-validation scores
print("Cross-validation scores:", scores)

# Calculate and print the mean accuracy
mean_accuracy = scores.mean()
print("Mean accuracy:", mean_accuracy)

from sklearn.decomposition import PCA

# Instantiate PCA with a desired number of components 
pca = PCA(n_components=100)

# Fit PCA on your training data
x_train_pca = pca.fit_transform(x_train['label_1'])

# Transform  validation and test data using the same PCA model
x_valid_pca = pca.transform(x_valid['label_1'])
x_test_pca = pca.transform(x_test['label_1'])  # If you have a test set


x_train_pca.shape

from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV, cross_val_predict
from sklearn.metrics import classification_report
from scipy.stats import uniform
from sklearn.model_selection import cross_val_score
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import GridSearchCV,HalvingGridSearchCV
# Define the hyperparameter grid or distribution
param_distributions = {
    'C': [0.1,1,10,100], #uniform(0.001, 100),  # Continuous uniform distribution for 'C'
    # 'kernel': ['rbf'],  # Experiment with different kernels
    # 'class_weight': ['balanced', None],  # Change class_weight options
    'gamma':  [0.001,0.01,0.1,1,'scale','auto']
    # 'degree': [1, 2, 3, 4]
}

# Instantiate the SVM classifier
svm_classifier = SVC(class_weight='balanced', kernel='rbf')


# Instantiate RandomizedSearchCV
random_search = RandomizedSearchCV(
    svm_classifier, param_distributions, n_iter=10,cv=5, scoring='accuracy', random_state=42,n_jobs=-1, verbose=1,
)

# # Perform cross-validation and get predictions
# cv_scores = cross_val_score(
#     random_search, x_train_pca, y_train['label_1'], cv=5, scoring='accuracy'
# )

# # Print cross-validation scores
# print("Cross-validation scores:", cv_scores)
# print("Mean accuracy:", cv_scores.mean())

# You can also print the classification report for each fold if needed
y_pred = cross_val_predict(
    random_search, x_train_pca, y_train['label_1'], cv=5, method='predict'
)
random_search.fit(x_train_pca, y_train['label_1'])

best_params = random_search.best_params_


# Evaluate the model
print("Best Hyperparameters:", best_params)
# Evaluate the model
print("Classification Report on Training Data (Cross-Validation):")
print(classification_report(y_train['label_1'], y_pred))

# Define the cross-validation strategy (e.g., StratifiedKFold with 5 folds)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

svm_classifier_l1 = SVC(class_weight='balanced' ,C=10, degree=1, gamma='auto', kernel='poly') #0.9160589060308556

# Perform cross-validation and calculate accuracy scores
scores = cross_val_score(svm_classifier_l1, x_train_pca_l1, y_train['label_1'], cv=cv, scoring='accuracy')
svm_classifier_l1.fit(x_train['label_1'], y_train['label_1'])

# Print the cross-validation scores and mean accuracy
print("Cross-validation scores:", scores)
print("Mean accuracy:", scores.mean())

# Define the cross-validation strategy (e.g., StratifiedKFold with 5 folds)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

svm_classifier_l1 = SVC(class_weight='balanced',C=100, gamma=0.001, kernel='rbf') #0.9359396914446003

# Perform cross-validation and calculate accuracy scores
scores = cross_val_score(svm_classifier_l1, x_train['label_1'], y_train['label_1'], cv=cv, scoring='accuracy')
svm_classifier_l1.fit(x_train['label_1'], y_train['label_1'])

# Print the cross-validation scores and mean accuracy
print("Cross-validation scores:", scores)
print("Mean accuracy:", scores.mean())

"""##Label 2

###SVM
"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Define the cross-validation strategy (e.g., StratifiedKFold with 5 folds)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define your SVM classifier with the desired hyperparameters
# svm_classifier = SVC(class_weight='balanced' ,C=10, degree=1, gamma='auto', kernel='poly') #=0.77321
svm_classifier = SVC(class_weight='balanced' ,C=1, kernel='rbf') #=0.77321


# Perform cross-validation and calculate accuracy scores
scores = cross_val_score(svm_classifier, x_train['label_2'], y_train['label_2'], cv=cv, scoring='accuracy')

# Print the cross-validation scores and mean accuracy
print("Cross-validation scores:", scores)
print("Mean accuracy:", scores.mean())

from sklearn.decomposition import PCA

# Instantiate PCA with a desired number of components (e.g., n_components=50)
pca = PCA(n_components=0.99)

# Fit PCA on your training data
x_train_pca = pca.fit_transform(x_train['label_2'])

# Transform your validation and test data using the same PCA model
x_valid_pca = pca.transform(x_valid['label_2'])
x_test_pca = pca.transform(x_test['label_2'])  # If you have a test set

# Now, you can use x_train_pca, x_valid_pca, and x_test_pca as your reduced-dimensional feature vectors for modeling.
x_train_pca.shape

from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Define the cross-validation strategy (e.g., StratifiedKFold with 5 folds)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define your SVM classifier with the desired hyperparameters
svm_classifier = SVC(class_weight='balanced' ,C=10, degree=1, gamma='auto', kernel='poly')

# Perform cross-validation and calculate accuracy scores
scores = cross_val_score(svm_classifier, x_train_pca, y_train['label_2'], cv=cv, scoring='accuracy')

# Print the cross-validation scores and mean accuracy
print("Cross-validation scores:", scores)
print("Mean accuracy:", scores.mean())

# Define the cross-validation strategy (e.g., StratifiedKFold with 5 folds)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# svm_classifier_l2 = SVC(class_weight='balanced' ,C=10, degree=1, gamma='auto', kernel='poly') # 0.755777460770328
svm_classifier_l2 = SVC(class_weight='balanced' ,C=100, gamma=0.001, kernel='rbf') #0.8853780313837376

# Perform cross-validation and calculate accuracy scores
scores = cross_val_score(svm_classifier_l2, x_train['label_2'], y_train['label_2'], cv=cv, scoring='accuracy')
svm_classifier_l2.fit(x_train['label_2'], y_train['label_2'])

# Print the cross-validation scores and mean accuracy
print("Cross-validation scores:", scores)
print("Mean accuracy:", scores.mean())

from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV, cross_val_predict,cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report
from scipy.stats import uniform

# Define the hyperparameter grid or distribution
param_distributions = {
    'C': [1,10,100],  # Continuous uniform distribution for 'C'
    'kernel': ['linear', 'rbf', 'poly'],  # Experiment with different kernels
    # 'class_weight': ['balanced', None],  # Change class_weight options
    'gamma': [0.001,0.01,1]
}

# Instantiate the SVM classifier
svm_classifier = SVC(class_weight='balanced')

# Instantiate RandomizedSearchCV
random_search = RandomizedSearchCV(
    svm_classifier, param_distributions, n_iter=10, cv=5, scoring='accuracy', random_state=42
)

# Perform cross-validation and get predictions
cv_scores = cross_val_score(
    random_search, x_train_pca, y_train['label_2'], cv=5, scoring='accuracy'
)

# Print cross-validation scores
print("Cross-validation scores:", cv_scores)
print("Mean accuracy:", cv_scores.mean())

# You can also print the classification report for each fold if needed
y_pred = cross_val_predict(
    random_search, x_train_pca, y_train['label_2'], cv=5, method='predict'
)

# Evaluate the model
print("Classification Report on Training Data (Cross-Validation):")
print(classification_report(y_train['label_2'], y_pred))

"""##Label 3"""

from sklearn.decomposition import PCA

# Instantiate PCA with a desired number of components (e.g., n_components=50)
pca = PCA(n_components=100)

# Fit PCA on your training data
x_train_pca = pca.fit_transform(x_train['label_3'])

# Transform your validation and test data using the same PCA model
x_valid_pca = pca.transform(x_valid['label_3'])
x_test_pca = pca.transform(x_test['label_3'])  # If you have a test set

# Now, you can use x_train_pca, x_valid_pca, and x_test_pca as your reduced-dimensional feature vectors for modeling.
x_train_pca.shape

"""###KNN"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=5)
classifier.fit(x_train['label_1'], y_train['label_3'])

y_pred = classifier.predict(x_valid['label_3'])

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_valid['label_3'], y_pred))
print(classification_report(y_valid['label_3'], y_pred))

"""###SVM"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Define the cross-validation strategy (e.g., StratifiedKFold with 5 folds)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define your SVM classifier with the desired hyperparameters
svm_classifier = SVC(class_weight='balanced' ,C=1, kernel='rbf')

# Perform cross-validation and calculate accuracy scores
scores = cross_val_score(svm_classifier, x_train['label_3'], y_train['label_3'], cv=cv, scoring='accuracy')

# Print the cross-validation scores and mean accuracy
print("Cross-validation scores:", scores)
print("Mean accuracy:", scores.mean())

from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Define the cross-validation strategy (e.g., StratifiedKFold with 5 folds)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define your SVM classifier with the desired hyperparameters
svm_classifier = SVC(class_weight='balanced' ,C=1, kernel='rbf')
# Best Hyperparameters: {'C': 12.216594343974382, 'degree': 1, 'kernel': 'poly'} balanced =0.98

# Perform cross-validation and calculate accuracy scores
scores = cross_val_score(svm_classifier, x_train_pca, y_train['label_3'], cv=cv, scoring='accuracy')

# Print the cross-validation scores and mean accuracy
print("Cross-validation scores:", scores)
print("Mean accuracy:", scores.mean())

from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV, cross_val_predict,cross_val_score, StratifiedKFold
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV,HalvingGridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from scipy.stats import uniform

# Define the hyperparameter grid or distribution
param_distributions = {
    'C': list(np.random.uniform(0.001, 100, 100)),  # Continuous uniform distribution for 'C'
    'kernel': ['linear', 'rbf', 'poly'],  # Experiment with different kernels
    'class_weight': ['balanced', None],  # Change class_weight options
    # 'gamma': ['scale', 'auto'],
    'degree': [1, 2, 3, 4]
}

# Instantiate the SVM classifier
svm_classifier = SVC()

# Instantiate RandomizedSearchCV
random_search = HalvingGridSearchCV(
    svm_classifier, param_distributions, cv=5, scoring='accuracy', random_state=42, factor=2, verbose=1, n_jobs=-1
)

# Perform cross-validation and get predictions

random_search.fit(x_train_pca, y_train['label_3'])
best_params = random_search.best_params_

# Evaluate the model
print("Best Hyperparameters:", best_params)


# You can also print the classification report for each fold if needed
y_pred = cross_val_predict(
    random_search, x_train_pca, y_train['label_3'], cv=5, method='predict'
)

# Evaluate the model
print("Classification Report on Training Data (Cross-Validation):")
print(accuracy_score(y_train['label_3'], y_pred))

from sklearn.metrics import accuracy_score
# Evaluate the model
print("Best Hyperparameters:", best_params)
print(accuracy_score(y_train['label_3'], y_pred))

"""###Cat Boost"""

pip install catboost

import numpy as np
import pandas as pd
import catboost
from catboost import CatBoostClassifier, Pool

# Assuming you have already loaded and preprocessed your data as x_train_pca, x_valid_pca, y_train, y_valid

# Instantiate CatBoostClassifier with desired hyperparameters
catboost_classifier = CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.1, loss_function='Logloss', cat_features=[])

# Create a CatBoost Pool object for training data
train_pool = Pool(data=x_train['label_3'], label=y_train['label_3'])

# Fit the CatBoost model on the training data
catboost_classifier.fit(train_pool)

# Make predictions on the validation data
y_pred = catboost_classifier.predict(x_valid['label_3'])

# Evaluate model performance (you can use scikit-learn's classification_report if needed)
from sklearn.metrics import classification_report
print(classification_report(y_valid['label_3'], y_pred))

import numpy as np
import pandas as pd
from catboost import CatBoostClassifier, Pool, cv
from sklearn.metrics import classification_report

# Assuming you have already loaded and preprocessed your data as x_train_pca, x_valid_pca, y_train, y_valid

# Define the hyperparameter grid for tuning
grid = {
    'iterations': [500, 1000, 1500],  # Number of boosting iterations
    'depth': [4, 6, 8],  # Depth of the trees
    'learning_rate': [0.05, 0.1, 0.15],  # Learning rate
}

# Create a CatBoost Pool object for training data
train_pool = Pool(data=x_train['label_3'], label=y_train['label_3'])

# Perform hyperparameter tuning using GridSearchCV
grid_search_result = catboost_classifier.grid_search(grid, train_pool, cv=5)

# Get the best hyperparameters
best_params = grid_search_result['params']

# Instantiate CatBoostClassifier with the best hyperparameters
best_catboost_classifier = CatBoostClassifier(**best_params, loss_function='Logloss', cat_features=[])

# Fit the CatBoost model on the training data
best_catboost_classifier.fit(train_pool)

# Make predictions on the validation data
y_pred = best_catboost_classifier.predict(x_valid['label_3'])

# Evaluate model performance
print("Best Hyperparameters:", best_params)
print("Classification Report on Validation Data:")
print(classification_report(y_valid['label_3'], y_pred))

"""##Label 4

###KNN
"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=5)
classifier.fit(x_train['label_4'], y_train['label_4'])

y_pred = classifier.predict(x_valid['label_4'])

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_valid['label_4'], y_pred))
print(classification_report(y_valid['label_4'], y_pred))

"""###SVM"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Define the cross-validation strategy (e.g., StratifiedKFold with 5 folds)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define your SVM classifier with the desired hyperparameters
svm_classifier = SVC(class_weight='balanced' ,C=1, kernel='rbf')

# Perform cross-validation and calculate accuracy scores
scores = cross_val_score(svm_classifier, x_train['label_4'], y_train['label_4'], cv=cv, scoring='accuracy')

# Print the cross-validation scores and mean accuracy
print("Cross-validation scores:", scores)
print("Mean accuracy:", scores.mean())

# Define the cross-validation strategy (e.g., StratifiedKFold with 5 folds)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# svm_classifier_l4 = SVC(class_weight='balanced' ,C=1, kernel='rbf') #0.8104493580599144
svm_classifier = SVC(class_weight='balanced' ,C=100, gamma=0.001, kernel='rbf') #0.9529243937232525

# Perform cross-validation and calculate accuracy scores
scores = cross_val_score(svm_classifier, x_train['label_4'], y_train['label_4'], cv=cv, scoring='accuracy')
svm_classifier.fit(x_train['label_4'], y_train['label_4'])

# Print the cross-validation scores and mean accuracy
print("Cross-validation scores:", scores)
print("Mean accuracy:", scores.mean())

from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Define the cross-validation strategy (e.g., StratifiedKFold with 5 folds)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define your SVM classifier with the desired hyperparameters
svm_classifier = SVC(class_weight='balanced' ,C=1, kernel='rbf')

# Perform cross-validation and calculate accuracy scores
scores = cross_val_score(svm_classifier, x_train_pca, y_train['label_4'], cv=cv, scoring='accuracy')

# Print the cross-validation scores and mean accuracy
print("Cross-validation scores:", scores)
print("Mean accuracy:", scores.mean())

from sklearn.svm import SVC
from scipy.stats import uniform
from sklearn.model_selection import RandomizedSearchCV, cross_val_predict,cross_val_score, StratifiedKFold
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV,HalvingGridSearchCV
from sklearn.metrics import classification_report


# Define the hyperparameter grid or distribution
param_distributions = {
    'C': list(np.random.uniform(0.001, 100, 10)),  # Continuous uniform distribution for 'C'
    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Experiment with different kernels
    # 'class_weight': ['balanced', None],  # Change class_weight options
    'gamma': ['scale', 'auto'],
    'degree': [1, 2, 3, 4]
}

# Instantiate the SVM classifier
svm_classifier = SVC(class_weight = 'balanced')

# Instantiate RandomizedSearchCV
random_search = HalvingGridSearchCV(
    svm_classifier, param_distributions, cv=5, scoring='accuracy', random_state=42, factor=2, verbose=1, n_jobs=7
)

# Perform cross-validation and get predictions
cv_scores = cross_val_score(
    random_search, x_train_pca, y_train['label_4'], cv=5, scoring='accuracy'
)

# Print cross-validation scores
print("Cross-validation scores:", cv_scores)
print("Mean accuracy:", cv_scores.mean())

# You can also print the classification report for each fold if needed
y_pred = cross_val_predict(
    random_search, x_train_pca, y_train['label_4'], cv=5, method='predict'
)

# Evaluate the model
print("Classification Report on Training Data (Cross-Validation):")
print(classification_report(y_train['label_4'], y_pred))

"""###Ramdom Fores"""

from sklearn.decomposition import PCA

# Instantiate PCA with a desired number of components (e.g., n_components=50)
pca = PCA(n_components=100)

# Fit PCA on your training data
x_train_pca = pca.fit_transform(x_train['label_4'])

# Transform your validation and test data using the same PCA model
x_valid_pca = pca.transform(x_valid['label_4'])
x_test_pca = pca.transform(x_test['label_4'])  # If you have a test set

# Now, you can use x_train_pca, x_valid_pca, and x_test_pca as your reduced-dimensional feature vectors for modeling.
x_train_pca.shape

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier  # You can replace this with any other model

model = RandomForestClassifier(n_estimators=100)
model.fit(x_train_pca, y_train['label_4'])  # Assuming x_train and y_train are your feature and target data

# Make predictions on the test data
y_pred = model.predict(x_valid_pca)

# Evaluate model performance
# print(classification_report(y_valid['label_4'], y_pred))

from sklearn.metrics import classification_report,accuracy_score
print(accuracy_score(y_valid_pca, y_pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],  # Number of trees in the forest
    'max_depth': [None, 10, 20, 30],  # Maximum depth of the trees
    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node
}

# Create a Random Forest classifier
rf_classifier = RandomForestClassifier()

# Perform Grid Search with cross-validation
grid_search = RandomizedSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)
grid_search.fit(x_train_pca, y_train['label_4'])

# Get the best model and hyperparameters
best_rf_classifier = grid_search.best_estimator_
best_params = grid_search.best_params_

# Make predictions on the validation set
y_pred = best_rf_classifier.predict(x_valid_pca)

# Evaluate the best model's performance
print("Best Hyperparameters:", best_params)
print("Accuracy on Validation Data:", accuracy_score(y_valid_pca, y_pred))
